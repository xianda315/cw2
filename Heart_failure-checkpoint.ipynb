{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5bc133",
   "metadata": {},
   "source": [
    "# Machine Learning Analysis for Early Detection of Heart Failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a941b-f607-466c-b6b7-325d30eec083",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c4bd1a-c3c3-4dd5-b2bb-7fe05b4368cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14810\\Desktop\\DSML\\DSML-main\\feature_implementation\\.ipynb_checkpoints\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data file not found: c:\\\\Users\\\\14810\\\\Desktop\\\\DSML\\\\DSML-main\\\\feature_implementation\\\\.ipynb_checkpoints\\\\..\\\\data\\\\heart.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing Script\n",
    "Author: deng.wei\n",
    "Date: 3.27\n",
    "Function: Preprocess the heart disease dataset, including handling outliers, categorical encoding, feature standardization, and data splitting.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "data_file = os.path.join(os.getcwd(), '..','data', 'heart.csv')\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check if the data file exists\n",
    "if not os.path.exists(data_file):\n",
    "    display(f\"Data file not found: {data_file}\")\n",
    "else:\n",
    "    try:\n",
    "        data = pd.read_csv(data_file)\n",
    "        print(f\"Data loaded successfully, shape: {data.shape}\")  # Expected shape is (918, 12)\n",
    "    except Exception as e:\n",
    "        display(f\"Error loading data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc249162-c265-42f4-9149-75c73f02866c",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df674b2-8f14-43fb-a229-47457bbd3e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data file not found: c:\\\\Users\\\\14810\\\\Desktop\\\\DSML\\\\DSML-main\\\\feature_implementation\\\\.ipynb_checkpoints\\\\..\\\\data\\\\heart.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing Script\n",
    "Author: deng.wei\n",
    "Date: 3.27\n",
    "Function: Preprocessing the heart disease dataset\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Go up one directory to access the dataset\n",
    "data_file = os.path.join(os.getcwd(), '..', 'data', 'heart.csv')\n",
    "# Check if the data file exists\n",
    "if not os.path.exists(data_file):\n",
    "    display(f\"Data file not found: {data_file}\")\n",
    "else:\n",
    "    try:\n",
    "        data = pd.read_csv(data_file)\n",
    "        print(f\"Data loaded successfully, shape: {data.shape}\")  # Expected shape is (918, 12)\n",
    "    except Exception as e:\n",
    "        display(f\"Error loading data: {e}\") \n",
    "    \n",
    "    # Handling outliers\n",
    "    '''\n",
    "    deng.wei: Blood pressure and cholesterol should not be zero\n",
    "    '''\n",
    "    bp_zero_count = (data['RestingBP'] == 0).sum()\n",
    "    chol_zero_count = (data['Cholesterol'] == 0).sum()\n",
    "    data['RestingBP'] = data['RestingBP'].replace(0, data['RestingBP'].median())\n",
    "    data['Cholesterol'] = data['Cholesterol'].replace(0, data['Cholesterol'].median())\n",
    "    print(\"Outlier handling completed\")\n",
    "    print(f\"Outlier handling: Replaced {bp_zero_count} zero values in blood pressure, {chol_zero_count} zero values in cholesterol\")\n",
    "    \n",
    "    '''\n",
    "    yue.yao: Copy global data for EDA analysis\n",
    "    '''\n",
    "    eda_data = data.copy()\n",
    "    \n",
    "    # Category encoding\n",
    "    categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "    print(\"Categorical feature encoding completed\")\n",
    "\n",
    "    # Splitting dataset\n",
    "    X = data.drop('HeartDisease', axis=1)\n",
    "    y = data['HeartDisease']\n",
    "    \n",
    "    # First, split into training and temporary sets (80% for training, 20% for temporary)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Then, split the temporary set into validation and test sets (each 10% of the original data)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size=0.5, \n",
    "        random_state=42, \n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset split completed:\")\n",
    "    print(f\"Training set {X_train.shape} ({len(X_train)/len(X):.1%})\")\n",
    "    print(f\"Validation set {X_val.shape} ({len(X_val)/len(X):.1%})\")\n",
    "    print(f\"Test set {X_test.shape} ({len(X_test)/len(X):.1%})\")\n",
    "    \n",
    "    # Standardizing numerical features\n",
    "    numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "    scaler = StandardScaler()\n",
    "    # Remove the line: data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "    # Because standardization is done only for training, validation, and test sets\n",
    "\n",
    "    # Standardizing the training set using its mean and standard deviation\n",
    "    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "\n",
    "    # Standardizing the validation and test sets using the training set's mean and standard deviation\n",
    "    X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
    "    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "    print(\"Numerical feature standardization completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132d4a5-8922-44b1-96de-63baaaf24e68",
   "metadata": {},
   "source": [
    "## Dataset Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b0f314-0078-4f62-a226-6800f4b83e1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mDataset Persistence\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mAuthor: deng.wei\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03mDate: 4.15\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Combine features and labels for export\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m train_full \u001B[38;5;241m=\u001B[39m \u001B[43mX_train\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m      8\u001B[0m train_full[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHeartDisease\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m y_train\n\u001B[0;32m     10\u001B[0m val_full \u001B[38;5;241m=\u001B[39m X_val\u001B[38;5;241m.\u001B[39mcopy()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset Persistence\n",
    "Author: deng.wei\n",
    "Date: 4.15\n",
    "\"\"\"\n",
    "# Combine features and labels for export\n",
    "train_full = X_train.copy()\n",
    "train_full[\"HeartDisease\"] = y_train\n",
    "\n",
    "val_full = X_val.copy()\n",
    "val_full[\"HeartDisease\"] = y_val\n",
    "\n",
    "# Not include the reult in test data\n",
    "test_full = X_test.copy()\n",
    "\n",
    "# Create 'report' folder if it doesn't exist\n",
    "report_dir = os.path.join(os.getcwd(), '..', 'report')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define output paths\n",
    "train_path = os.path.join(report_dir, f\"train_set_snapshot_.csv\")\n",
    "val_path   = os.path.join(report_dir, f\"val_set_snapshot_.csv\")\n",
    "test_path  = os.path.join(report_dir, f\"test_set_snapshot_.csv\")\n",
    "\n",
    "train_full.to_csv(train_path, index=False)\n",
    "val_full.to_csv(val_path, index=False)\n",
    "test_full.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"Dataset snapshots saved\")\n",
    "print(f\"Training set: {train_path}\")\n",
    "print(f\"Validation set: {val_path}\")\n",
    "print(f\"Test set: {test_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008f57d-ff31-4803-952d-0863a4001349",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153877d-4494-41af-b381-c5e72363377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "探索性分析数据EDA\n",
    "作者: yue.yao\n",
    "日期: 4.6\n",
    "功能: 对心脏疾病数据集进行探索性分析，完成数据可视化\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 数据全局概览\n",
    "eda_data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测空值\n",
    "eda_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3684c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测重复数据\n",
    "eda_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 概览非数值数据分布\n",
    "for i in eda_data.columns:\n",
    "  if eda_data[i].dtype == 'object':\n",
    "    print(f'column : {i}')\n",
    "    print(eda_data[i].value_counts())\n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数值数据进行相关性分析\n",
    "num_cor = eda_data.corr(numeric_only=True)\n",
    "num_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制相关性热力图\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(num_cor, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d622851",
   "metadata": {},
   "source": [
    "> 结论：  \n",
    "与HeartDisease相关性较高的数据是Oldpeak，FastingBS，Age  \n",
    "> Conclusion:  \n",
    "The data with high correlation with HeartDisease are Oldpeak, FastingBS, and Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b39174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比Oldpeak对于患病和非患病人群的平均数值分布\n",
    "print(eda_data.groupby('HeartDisease')['Oldpeak'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b08f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 箱型图对比Oldpeak数值分布\n",
    "sns.boxplot(x='HeartDisease', y='Oldpeak', data=eda_data, hue='HeartDisease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9476e4",
   "metadata": {},
   "source": [
    "> 结论：  \n",
    "心力衰竭患者的Oldpeak指数通常比非患者要高，数值大多分布在1.5及以上  \n",
    "Conclusion:  \n",
    "The Oldpeak index was generally higher in patients with heart failure than in non-patients, with values of 1.5 and above mostly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb34276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 患者和非患者的年龄分布\n",
    "sns.histplot(\n",
    "    data=eda_data, \n",
    "    x='Age', \n",
    "    hue='HeartDisease',\n",
    "    bins=30, \n",
    "    kde=True, \n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"Age Distribution by Heart Disease Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5980e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 箱型图对比Age数值分布\n",
    "sns.boxplot(x='HeartDisease', y='Age', data=eda_data, hue='HeartDisease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 静息血压岁年龄变化\n",
    "sns.lineplot(x='Age', y='Oldpeak', data=eda_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e45629",
   "metadata": {},
   "source": [
    "> 结论：  \n",
    "Oldpeak数值随年龄增大而增加。心力衰竭患者数量随年龄上升而增加，大多患者年龄分布在60岁左右。  \n",
    "Conclusion:  \n",
    "Oldpeak value increases with age. The number of heart failure patients increases with age, and most patients are around 60 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 箱型图对比FastingBS数值分布\n",
    "sns.boxplot(x='HeartDisease', y='FastingBS', data=eda_data, hue='HeartDisease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996940e",
   "metadata": {},
   "source": [
    "> 结论：  \n",
    "心力衰竭患者普遍能检测到FastingBS数值分布在0-1之间，而非患者普遍数值为0。FastingBS数值与患病情况强相关。  \n",
    "Conclusion:  \n",
    "Patients with heart failure generally had FastingBS values ranging from 0 to 1, while non-patients generally had values of 0. FastingBS values were strongly correlated with disease status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205efb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非数值数据的患病分布\n",
    "categoricalfeatures = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in categoricalfeatures:\n",
    "    ax = plt.subplot(2,3,categoricalfeatures.index(i)+1)\n",
    "    sns.countplot(x= eda_data[i] , hue = eda_data['HeartDisease'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13531b6",
   "metadata": {},
   "source": [
    "> 结论：  \n",
    "1.心力衰竭患者在男性中占比更高，在女性中占比低。  \n",
    "2.在几种胸痛类型中，患者表现出无症状的数量最多。  \n",
    "3.RestingECG在患者和非患者间表现出数值相差不明显。  \n",
    "4.在患者中，运动诱发性心绞痛占比例较高。  \n",
    "5.根据ST_Slope数值显示，患者大多ST曲线表现为水平型。  \n",
    "Conclusion:  \n",
    "1.Heart failure is more common in men and less common in women.  \n",
    "2.Of the several types of chest pain, the largest number of patients showed no symptoms.  \n",
    "3.RestingECG showed no significant difference in values between patients and non-patients.  \n",
    "4.Exercise-induced angina pectoris accounted for a high proportion of patients.  \n",
    "5.According to the value of ST_Slope, the ST curve of most patients is horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bcb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散点图矩阵\n",
    "sns.pairplot(eda_data,hue='HeartDisease')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd3f6a",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2b290-8215-4d2e-93a4-59c9b9e085cb",
   "metadata": {},
   "source": [
    "### Public function(Making code more elegent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d84f3-e26a-4df3-9403-edbb11696bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: deng.wei\n",
    "Date: 4.15\n",
    "Function: To generate metrix images by elegant way\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\", labels=[\"0\", \"1\"]):\n",
    "    \"\"\"\n",
    "    Function to plot confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    cm : ndarray\n",
    "        Confusion matrix to be plotted.\n",
    "    title : str\n",
    "        Title of the plot.\n",
    "    labels : list\n",
    "        List of labels for the confusion matrix (default is [\"0\", \"1\"]).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f959e-a53e-4089-a0b6-3dad40526324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: deng.wei\n",
    "Date: 4.15\n",
    "Function: To generate evaluation indicators by elegant way(WITHOUT validation_set)\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "\n",
    "def evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred):\n",
    "    print(\"Training Set:\")\n",
    "    print(f\"Accuracy     : {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"Precision    : {precision_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"F1 Score     : {f1_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"Recall       : {recall_score(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set:\")\n",
    "    print(f\"Accuracy     : {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"Precision    : {precision_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"F1 Score     : {f1_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"Recall       : {recall_score(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51119f7f-dfdc-4dcd-9197-d535ba886300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: deng.wei\n",
    "Date: 4.15\n",
    "Function: To generate evaluation indicators by elegant way(INCLUDING validation_set)\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "def evaluate_model_outputs(y_train, y_train_pred, y_val, y_val_pred, y_test, y_test_pred):\n",
    "    print(\"Training Set:\")\n",
    "    print(f\"Accuracy     : {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"Precision    : {precision_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"F1 Score     : {f1_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"Recall       : {recall_score(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "    print(\"\\nValidation Set:\")\n",
    "    print(f\"Accuracy     : {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"Precision    : {precision_score(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"F1 Score     : {f1_score(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"Recall       : {recall_score(y_val, y_val_pred):.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set:\")\n",
    "    print(f\"Accuracy     : {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"Precision    : {precision_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"F1 Score     : {f1_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"Recall       : {recall_score(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380b8c2-6b4f-4762-a15a-444beeecabf6",
   "metadata": {},
   "source": [
    "### 1.KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd6d1d-bdc5-49f3-9108-f83de1963fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Create and train the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"\\nCross-Validation (5-Fold):\")\n",
    "print(f\"CV Scores    : {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Tune K value using CV\n",
    "k_range = range(1, 21)\n",
    "cv_results = []\n",
    "for k in k_range:\n",
    "    knn_k = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(knn_k, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    cv_results.append(score)\n",
    "\n",
    "best_k = k_range[cv_results.index(max(cv_results))]\n",
    "print(f\"\\nBest K value: {best_k}\")\n",
    "\n",
    "# Retrain with best K\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_best.fit(X_train, y_train)\n",
    "y_test_pred_best = knn_best.predict(X_test)\n",
    "print(f\"Test set accuracy after tuning: {accuracy_score(y_test, y_test_pred_best):.4f}\")\n",
    "\n",
    "# Confusion matrices\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ca5a3-bead-4bd6-ab69-dd3c9ce04f43",
   "metadata": {},
   "source": [
    "### 2.Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aceef55-4f71-4519-ad45-06513ef4a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decision Tree Model\n",
    "Author: deng.wei\n",
    "Date: 4.14\n",
    "Function: Decision Tree model implementation and evaluation\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split=10, min_samples_leaf=5)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = dt.predict(X_train)\n",
    "y_test_pred = dt.predict(X_test)\n",
    "\n",
    "evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff297c7b-48a8-49aa-91ca-82aa26396897",
   "metadata": {},
   "source": [
    "### 3.Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd21f2-e1ba-4d49-babe-dd2b2926fd2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random Forest Model\n",
    "Author: deng.wei\n",
    "Date: 4.13\n",
    "Function: Random Forest model implementation and evaluation\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=42, \n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=3\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_val_pred = rf.predict(X_val)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate with your shared evaluation function (train + val + test)\n",
    "evaluate_model_outputs(y_train, y_train_pred, y_val, y_val_pred, y_test, y_test_pred)\n",
    "\n",
    "# Cross-validation (10-fold)\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=10, scoring='accuracy') \n",
    "print(\"\\nCross-Validation (10-Fold):\")\n",
    "print(f\"CV Scores    : {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Confusion matrices\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1315d24-a81b-465f-a60d-1b15d35daec2",
   "metadata": {},
   "source": [
    "### 4.Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7852be-5594-4fb6-9ac2-28db79790980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic Regression Model\n",
    "Author: deng.wei\n",
    "Date: 4.14\n",
    "Function: Logistic Regression model implementation and evaluation\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate using shared utility (no validation set)\n",
    "evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "# Confusion matrices\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22100b96-be46-4cca-be5d-0d73fb827b38",
   "metadata": {},
   "source": [
    "### 5.SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971be0a-75bf-4acd-ac14-541eda792d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVM Model  \n",
    "Author: deng.wei  \n",
    "Date: 4.14  \n",
    "Function: Implementation and evaluation of the SVM model\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svm.predict(X_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "\n",
    "evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39dfe4-63ba-4f6b-8ac8-a11b2a5a9d71",
   "metadata": {},
   "source": [
    "### 6.Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b995efe-abbe-4dc4-9518-98c509d9c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Naive Bayes Model  \n",
    "Author: deng.wei  \n",
    "Date: 4.14  \n",
    "Function: Implementation and evaluation of the Naive Bayes model\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = nb.predict(X_train)\n",
    "y_test_pred = nb.predict(X_test)\n",
    "\n",
    "evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba8ba7-c4f9-4cba-8d4d-15bc7ae2ab7a",
   "metadata": {},
   "source": [
    "### 7.Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef4e34-6263-4a7c-81fc-146176b23a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural Network Model  \n",
    "Author: deng.wei  \n",
    "Date: 4.14  \n",
    "Function: Implementation and evaluation of a neural network model\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a neural network model with regularization and early stopping\n",
    "mlp = MLPClassifier(\n",
    "    max_iter=1000,\n",
    "    learning_rate_init=0.001,\n",
    "    alpha=0.001,            # L2 regularization\n",
    "    early_stopping=True,    # Enable early stopping\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = mlp.predict(X_train)\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "cross_val_scores = cross_val_score(mlp, X_train, y_train, cv=5)\n",
    "print(f\"\\nCross-validation accuracy scores: {cross_val_scores}\")\n",
    "print(f\"Mean cross-validation accuracy: {cross_val_scores.mean():.4f}\")\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21fa52-49a7-4d28-97da-947b88e4f892",
   "metadata": {},
   "source": [
    "### 8.Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7995292-1676-4048-b46d-0cbfcd700904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Descent Model  \n",
    "Author: deng.wei  \n",
    "Date: 4.15  \n",
    "Function: Implementation and evaluation of a gradient-based model\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "sgd = SGDClassifier(loss='log_loss', random_state=42)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = sgd.predict(X_train)\n",
    "y_test_pred = sgd.predict(X_test)\n",
    "\n",
    "evaluate_model_basic(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm_train, title=\"Confusion Matrix for Training Set\")\n",
    "plot_confusion_matrix(cm_test, title=\"Confusion Matrix for Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c87642-72a6-4fe2-8ea9-80a4f47a34ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7c4d9-0cd1-4287-ae1e-b8989f50f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee97b1-ff4c-44ca-adbc-d61378d0e1e9",
   "metadata": {},
   "source": [
    "## Predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e6781-0c5a-42bc-a384-711cc8033a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择合适的模型后进行结果预测"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T07:53:45.862574Z",
     "start_time": "2025-04-24T07:53:45.860543Z"
    }
   },
   "id": "84bd53e5a6d07d7b",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
